# 实验报告

## 第一阶段 豆瓣数据的检索

### 一、分词

#### **实验目标：**

​	对一阶段中给定的电影和书籍数据进行预处理，选择一个合适的分词算法，将文本表征为关键词集合，并去除其中停用词以及通过词语的语 义相似度来合并同义词，比较不同方法的性能差异。

#### **实验方案：**

代码文件`cut.py`

- 分词处理：分别采用jieba以及THULAC两种分词工具对原数据集进行分词处理，得到关键词表。
- 停用词处理：根据 [哈工大停用词表](https://github.com/goto456/stopwords)（hit_stopwords.txt）去除关键词表中的停用词。
- 同义词合并：根据手工标注的[同义词表](https://github.com/guotong1988/chinese_dictionary)（dict_synonym.txt）对去除停用词后的关键词表进行同义词合并操作：对每个关键词，如果其处于同义词表内，则将其归一化为改词项中第一个词，并在后续的查询中，对查询词作同样的归一化处理。

#### **实验结果及分析：**

**关键词文件：**

`book_keywords_raw.csv`:未经停用词及同义词处理得到的书籍关键词表

`book_keywords.csv`:经停用词及同义词处理得到的书籍关键词表

`movie_keywords_raw.csv`:未经停用词及同义词处理得到的电影关键词表

`movie_keywords.csv`:经停用词及同义词处理得到的电影关键词表

**分词工具比较：**

- **性能：**[0]

  msr_test（560KB）：

  | Algorithm | Time  | Precision | Recall |
  | --------- | ----- | --------- | ------ |
  | jieba     | 0.26s | 0.814     | 0.809  |
  | THULAC    | 0.62s | 0.877     | 0.899  |

  pku_test（510KB）：

  | Algorithm | Time  | Precision | Recall |
  | --------- | ----- | --------- | ------ |
  | jieba     | 0.23s | 0.850     | 0.784  |
  | THULAC    | 0.51s | 0.944     | 0.908  |

  CNKI_journal.txt（51 MB）：

  | Algorithm | Time     | Speed       |
  | --------- | -------- | ----------- |
  | jieba     | 22.5583s | 2314.89KB/s |
  | THULAC    | 42.625s  | 1221.05KB/s |

- **基于词表的分析：**

  以书籍为例，对比两种分词工具得到的分词表`book_keywords_raw.csv`:

  1. 在对专有名词的识别上，jieba的处理要优于THULAC，能识别出诸如“村上”、“浮世绘”等专有名词
  2. 在分词的粒度上，对比关键词表的大小可以发现，jieba分词（3249KB）的粒度要高于THULAC（2749KB），更高的粒度往往意味着搜索时更好匹配。
  3. 在对分词处理方面，THULAC内置了去除停用词，繁体简化，以及词性标明等选项，有助于文本的分析。

- **总结：**

  ​	由于本实验目的是为了实现关键词的检索，不需要进行词性分析，更注重于分词的粒度以及专有名词的识别，而jieba分词在这些性能上较THULAC更优，故采用jieba的关键词表进行后续的倒排索引表建立以及检索操作。

  

**去除停用词、合并同义词前后关键词表比较：**（以书籍为例）

- 关键词文件大小：3249KB $\rightarrow$ 3000KB

  停用词的去除使得关键词文件减小，同时由于停用词的频度往往较高，去除后更易于后续建立倒排索引表

- 顺序倒排索引表大小：3103KB $\rightarrow$ 3033KB

  同义词的合并使得倒排索引表的大小进一步缩小，同时有助于使得查询条件更加宽泛，提高检索成功率。



### 二、倒排索引表

#### **实验目标：**

​	基于前一阶段形成的分词结果，在经过预处理的数据集上建立倒排索引表， 并以合适的方式存储生成的倒排索引文件。

#### **实验方案：**

代码文件`index-table.py` `index-table with term.py`

- **倒排索引表建立**：

  依据课程中介绍的三部走策略，完成倒排索引表的建立

  1. 检索每篇文档，获得<词项，文档ID>对，并写入临时索引`csv_to_index_table(csvfile)`
  2. 对临时索引中的词项进行排序`sort_index_table(index_table):`
  3. 遍历临时索引，对于相同词项的文档ID进行合并`csv_to_index_table(csvfile)`

  *注：由于写入临时索引时，就已经对相同词项的文档ID进行了合并，实际上第一步和第三步是同时执行的。*

- **跳表指针的建立：**

  在原倒排索引表的基础上，建立跳表指针，方便查询时快速进行合并操作`skip_list_build(index_table)`

  跳表指针设计：

  ​	采用单层跳表结构，每隔$\sqrt L$设置一个指针。这里跳表采用的数据结构是：并不真正增加指针域，而是在原有倒排索引表的基础上每隔$\sqrt L$取一元素组成新的列表，在查询时依据辅助函数`skip_index(key)`获取原列表位置索引即可。

- **倒排索引表的存储：**

  将倒排索引表存储进文件，采用两种方案：

  1. **顺序倒排索引表，其中词项和索引表一起存储：**

     转化为csv格式：`write_index_table_to_csv(file,index_table)`

     利用shelve模块存储：`index_table_storage(index_table,file_index_list)`

  2. **hash倒排索引表，其中词项和索引表分开存储：**

     `hash_function(input_str)`是选用的hash散列函数，具体设计为：

     ​	选择质数因子为31，将词项每个字符的字节值与hash值和质数因子乘积相加，再用位操作映射到限制	范围中以防溢出，依次迭代。然后将所得的hash值映射到（0，81919）中。

     `hash_index_table_storage(index_table,file_hash_list,file_index_list)`为对应存储函数：

     ​	对于词项，建立hash表，将每个词项同其原来的顺序索引一起存入对应hash值的表项中，查询时只需	对查询词进行同样的hash处理后，找到该hash对应词项，根据顺序索引访问倒排表即可。hash表采用	pkl文件格式存储，方便后续查询时直接访问。

     ​	对于倒排索引，建立顺序-索引表键值对，利用shelve模块存入文件中。采用shelve模块的好处是，实	际应用中往往倒排索引较为庞大，不能全部载入内存，通过shelve模块可以实现根据索引直接部分访问	文件得到查询结构，内存开销大大减小，但这不可避免地导致查询速率的降低。

  **注：**之所以采用hash存储，是出于对数据文件特性的考量：大量的中文文本（相同前缀较少）会导致Tier树的分支极多，对空间压缩以及查询速率提高帮助有限，同时Tier树以及B+树处理中文的算法较为复杂，可能会进一步降低算法效率。而对于hash存储，中英文没有本质区别，算法实现简单，字典式的搜索能够提高检索的效率。

#### **实验结果及分析：**

**数据文件：**

顺序倒排索引表：

`book_index_table.csv` csv格式的书籍顺序倒排索引表

`book_index_list_with_term` shelve模块存储的书籍顺序倒排索引表

`movie_index_table.csv` csv格式的电影顺序倒排索引表

`movie_index_list_with_term` shelve模块存储的电影顺序倒排索引表

hash倒排索引表：

`book_hash_list.pkl` 书籍hash表

`book_index_list` 书籍倒排索引表

`movie_hash_list.pkl` 电影hash表

`movie_index_list` 电影倒排索引表



### 三、布尔查询

#### 实验目标：

​	优化生成的倒排索引表，对于给定的包含任意组合（如括号）的布尔查询（例如 (动作 and 剧情) or (科幻 and not 恐怖)），使其能够支持复杂的布尔查询操作。返回符合查询规则的电影或/书籍集合，并以合适的方式展现给用户（例如输出 tag等）。分析不同词项处理顺序以及倒排索引表存储方式对查询效率的影响。

#### 实验方案:

代码文件

`bool-query.py` 载入顺序倒排索引表的布尔查询

`bool-query unload.py`文件访问顺序倒排索引表的布尔查询

`bool-query with hash.py`基于hash倒排索引表的布尔查询

`bool-query with hash better.py`基于hash倒排索引表，优化了词项处理顺序的布尔查询

- **基础bool查询函数建立：**

  *注：合并使用了跳表，通过两个索引（原索引和跳表索引）的交叉递进操作完成词项在列表上的查询操作，*

  *将满足条件的词项添加到合并列表中，完成后再在列表上建立跳表（详见代码文件）。*

  `merge_AND(list_key_a, list_key_b)`：基于逻辑与操作，对a，b两个词项列表进行取交集操作。

  `merge_OR(list_key_a, list_key_b)`：基于逻辑或操作，对a，b两个词项列表进行取并集操作。

  `search_NOT(list_key_a,all_docs)`：基于逻辑非操作，返回除了a词项列表之外的所有文档id。

  `merge_ANDNOT(list_key_a, list_key_b)`：and和not的符合逻辑，考虑到取非有着更高的优先级，先进行取非操作再进行合并势必会导致操作量急剧增加，因此用andnot复合逻辑来代替，只需在a的基础上剔除b中存在的id即可。

- **复合bool表达式的解析：**

  构建查询解析器类以完成复合bool表达式的解析

  **class BooleanQueryParser:**

  __init__(files):载入必要的文档。

  `tokenize(self, query)`：使用正则表达式分割查询字符串为tokens

  `parse(self, tokens)`：递归下降解析器，对每个逻辑关键字，按照优先级(低到高)顺序递归下降。

  `parse_expr(tokens)`：表达式层，处理逻辑OR。

  `parse_term(tokens)`：词项层，处理逻辑AND和ANDNOT。

  `parse_factor(tokens)`：因子层，处理括号和逻辑NOT，并递归处理下层表达式。当达到底层时执行查询操作。

- **词项的查询：**

  对于顺序存储：

  直接在已载入的倒排索引表或者文件中的倒排索引表上进行比较操作，返回表项即可。

  对于hash存储：

  首先要在初始化的时候将hash表载入内存：`hash_load(file_hash_list)`

  然后在解析底层执行查询操作时，调用`hash_search(word,hash_list,file_index_list)`，将词项映射为hash值后在hash表上查找，得到索引后再访问倒排索引表文件，返回表项。

- **词项处理顺序优化**：

  利用课程中提到的，对于AND操作，总是先合并文档频率较小的词项，以减少总体复杂度。

  采取措施为：

  在词项层，先不执行合并操作，而是将所有AND词项和ANDNOT词项存入缓存列表。

  再利用`process_and_operations(self, factors)`函数，完成词项合并：

  对缓存列表进行排序，每次选取AND词项中文档频率最小的一项与已处理项（不会大于“最小”项的长度）进行合并操作，直到全部取出，得到AND合并项。然后对ANDNOT词项进行OR操作（这样可以避免多次进行ANDNOT）得到ANDNOT合并项，两项进行ANDNOT操作即得最终合并项。

#### 实验结果及评估

对于三种不同bool表达式（前两个查询书籍，第三个查询电影），四种不同查询，执行结果如下：

载入顺序倒排索引表的布尔查询

![image-20241117143821097](asserts\bool query.png)

文件访问顺序倒排索引表的布尔查询

![image-20241117144039397](asserts\bool query u.png)

基于hash倒排索引表的布尔查询

![image-20241117144137423](asserts\bool query h.png)

基于hash倒排索引表，优化了词项处理顺序的布尔查询

![image-20241117144259879](asserts\bool query hb.png)

分析：

从结果分析得出：

1. 由于未载入索引表，查询效率会降低，好处是内存占用大大降低。
2. 采用hash存储的索引表在查询上显著优于顺序存储的索引表，且随着查询复杂度增加优势更加明显。
3. 对于词项顺序的处理优化没有提高查询速率，甚至不如优化前，可能原因有：
   - 词项过少，使得优化没有那么明显，反而因算法复杂度提高导致速率降低。
   - 数据集较小，每个词项对应的文档数可能相差不大，处理顺序影响不大。
   - 算法优化不足，由于对文档排序时采用了合并文档大小，导致排序前需完成所有子文档的合并，倘若通过文档频率预测方式进行排序，可能结果会有所优化。



### 四、倒排索引表压缩

#### 实验目标：

​	任选两种课程中介绍过的索引压缩方法加以实现，如按块存储、前端编码等， 并比较压缩后的索引在存储空间和检索效率上与原索引的区别

#### 实验方案：

代码文件:

`compress.py`：两种存储压缩

`bool-query with hash better after compress`：压缩后进行查询

1. **词项哈希表按块存储压缩**

   对于词项，采用按块存储的方式进行压缩：

   `compress_terms(term_hash_table)`:词项压缩函数

   按顺序遍历hash表，对每个hash值下的多个词项列表，将词项按照gb18030编码为字节串，再以字节长度+字节串格式存入数据块block中，处理完该hash值下的所有词项后，添加块指针到指针数组。

   `decompress_terms(term_hash_table)`:词项解压缩函数

   利用指针数组访问block中每个hash值的数据块，根据词项字节长度还原每个词项，重建hash表。

2. **倒排索引表文档间距变长编码存储压缩**

   对于文档id，通过取间距，再进行变长编码进行压缩：

   `compress_ids(id_list)`：文档id压缩函数

   对于每个文档id列表，先将id全部由字符串转化为整型变量，再以第一个id为基准id，后续id分别取与前一项的差值（间距），得到间距处理后的文档id列表。但为了利用上文档id间距较小的特点，还需要采用变长编码才能完成压缩。

   `varint_encode(value)`：变长编码函数

   将每128（7位）范围的数加上一个延续位（1位）编码成一字节，最大化利用了空间。

   `varint_decode(value)`：变长编码对应的解码函数

   `decompress_ids(id_list)`：文档id压缩函数

   对变长编码解码后得到间距，再在基准id的基础上依次还原得到原始id列表。

#### 实验结果及分析：

hash表压缩：

![image-20241117152241814](asserts\hash-compress.png)

索引表压缩：

![image-20241117152140189](asserts\index-compress.png)

压缩后进行查询：

![image-20241117152400401](asserts\bool query compress.png)

**分析：**

hash表压缩：

通过按块存储的方式，书籍及电影hash表分别压缩到原来的42.34%、49.17%，效果显著。

索引表压缩：

通过文档间距变长编码的方式，书籍及电影索引表分别压缩到原来的25.20%、25.48%，效果显著。

压缩后进行查询：

可以看到，相较于未压缩前的基于hash倒排索引表，优化了词项处理顺序的布尔查询，压缩后的查询速率甚至有所提高，推测是因为由于只用部分访问数据，解压的时间消耗不多，而在压缩后的文件上的查找操作效率大大提高，因此bool表达式的查询速率有所提高。



### 总结：

​	实验完成了所有实验要求:

- 采用了两种分词工具（jieba、THULAC）对原数据集进行了分词处理，并比较了两者的效果。对分词结果实现了停用词的去除以及同义词的合并，并分析了前后的关键词表的差别。

- 根据课程中的三步走策略建立了倒排索引表及跳表，并通过顺序和hash两种存储方式将索引表存入文件。

- 在此基础上，分析比较了不同存储方式，以及不同词项处理顺序下的复合bool表达式的查询效率。

- 最后，分别对hash表和索引表采用了两种课程中提到的压缩策略（词项哈希表按块存储压缩，倒排索引表文档间距变长编码存储压缩），并比较分析了压缩前后的文件大小以及查询速率。



### 相关说明：

[0] 数据来源：

[THAULAC分词软件性能对比]: https://github.com/thunlp/THULAC-Python)

