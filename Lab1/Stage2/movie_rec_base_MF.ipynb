{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是基于`Matrix Factorization`的加入正则项的普通 \\( MSE \\)的协同过滤算法来进行的电影评分的预测，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sklearn 库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_score.csv readed......\n"
     ]
    }
   ],
   "source": [
    "# 读取 movie_score_csv\n",
    "file_path = ('data/movie_score.csv')\n",
    "read_data = pd.read_csv(file_path)\n",
    "print('movie_score.csv readed......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#用户和电影的个数，构建 P Q 矩阵需要\n",
    "users_nums = len(read_data['User'].unique())\n",
    "movies_nums = len(read_data['Movie'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己的电影评分数据集类，返回用户 id的index ，电影 id的index , 以及对应的评分\n",
    "class MovieScoreDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, read_data, all_dataset = None):\n",
    "        self.read_data = read_data\n",
    "        if all_dataset == None:\n",
    "            #依据字典索引得到 user 和 movie 的 id 列表\n",
    "            self.users_unique_id_list = read_data['User'].unique()\n",
    "            self.movies_unique_id_list = read_data['Movie'].unique()\n",
    "            self.users_unique_id_list = sorted(set(self.users_unique_id_list))\n",
    "            self.movies_unique_id_list = sorted(set(self.movies_unique_id_list))\n",
    "            #依据id列表，创建 user 和 movie 的分别从 id 到 index 的转换，便于实现利用索引访问对应的矩阵 factor\n",
    "            self.user_id_to_index = {id : index for index, id in enumerate(self.users_unique_id_list)}\n",
    "            self.movie_id_to_index = {id : index for index, id in enumerate(self.movies_unique_id_list)}\n",
    "        else:\n",
    "            self.user_id_to_index = all_dataset.user_id_to_index\n",
    "            self.movie_id_to_index = all_dataset.movie_id_to_index\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 得到数据文件中index对应的一行\n",
    "        one_row = self.read_data.iloc[index]\n",
    "        user_index = self.user_id_to_index[one_row['User']]\n",
    "        movie_index = self.movie_id_to_index[one_row['Movie']]\n",
    "        u_b_rating = one_row['Rate'].astype('float32')\n",
    "        #返回 index\n",
    "        return user_index, movie_index, u_b_rating\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#划分数据集\n",
    "train_data, test_data = train_test_split(read_data, test_size=0.5, random_state=42)\n",
    "\n",
    "#创建数据集\n",
    "all_dataset = MovieScoreDataSet(read_data)\n",
    "train_dataset = MovieScoreDataSet(train_data, all_dataset)\n",
    "test_dataset = MovieScoreDataSet(test_data, all_dataset)\n",
    "\n",
    "#创建训练和测试数据迭代器，批量大小设置为256\n",
    "batch_size = 256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基础的基于MSE损失的 MF 模型\n",
    "class BaseMSEMFModel(nn.Module):\n",
    "    def __init__(self, factor_dim, users_nums, movies_nums):\n",
    "        super(BaseMSEMFModel, self).__init__()\n",
    "        self.user_matrix = nn.Embedding(users_nums, factor_dim)\n",
    "        self.movie_matrix = nn.Embedding(movies_nums, factor_dim)\n",
    "\n",
    "    # 前向传播函数，得到预测评分值   \n",
    "    def forward(self, X):\n",
    "        user_index, movie_index = X\n",
    "        user_vector = self.user_matrix(user_index)\n",
    "        movie_vector = self.movie_matrix(movie_index)\n",
    "        # 经过Embedding层出来的vector形状应该是（batch_size x factor_dim）\n",
    "        # 做点积，得到（batch_size X 1）的预测评分\n",
    "        return (user_vector * movie_vector).sum(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(net, loss, train_iter, test_iter, batch_size, device, num_epochs = 100, lr = 0.02):\n",
    "    # 定义优化器为 SGD \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    # 正则化超参数\n",
    "    lambda_1 = 0.001\n",
    "    lambda_2 = 0.001\n",
    "    # 迭代训练，每一个 epoch 打印 train loss 、test loss and test ndcg_score\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        for i, X in enumerate(train_iter):\n",
    "            user_index, movie_index, u_b_rating = X \n",
    "            # 梯度清零\n",
    "            trainer.zero_grad()\n",
    "            X = (user_index.to(device),movie_index.to(device))\n",
    "            # 模型预测\n",
    "            hat_rate = net(X)\n",
    "\n",
    "            # 计算损失，加入L2范数进行正则化\n",
    "            l = loss(hat_rate, u_b_rating.to(device)).mean() + lambda_1 * net.user_matrix.weight.norm(2) \n",
    "            + lambda_2 * net.movie_matrix.weight.norm(2)\n",
    "\n",
    "            train_loss += l\n",
    "            # 反向传播\n",
    "            l.backward()\n",
    "            # 更新参数\n",
    "            trainer.step()\n",
    "\n",
    "        train_loss /= i+1\n",
    "\n",
    "        # 测试评估\n",
    "        net.eval()\n",
    "        test_loss = 0.0\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for i, X in enumerate(test_iter):\n",
    "                user_index, movie_index, u_b_rating = X\n",
    "                X = (user_index.to(device),movie_index.to(device))\n",
    "                #得到预测得分\n",
    "                predict_rate = net(X)\n",
    "                #计算损失\n",
    "                l = loss(predict_rate, u_b_rating.to(device)).mean()\n",
    "                test_loss += l\n",
    "\n",
    "                #下面计算测试集的ndcg_socre，来评估预测的排序的效果\n",
    "                res = torch.cat([user_index.unsqueeze(1), predict_rate.cpu().unsqueeze(1), u_b_rating.unsqueeze(1)], dim = 1)\n",
    "                results.append(res)\n",
    "\n",
    "            # results变为一个(num_test_instance, 3)的张量\n",
    "            results = torch.stack(results, dim = 0)\n",
    "            results = results.flatten(start_dim=0, end_dim=1)\n",
    "            # 对不同用户分组，分别计算ndcg\n",
    "            # 将 user_index, predict_rate 和 u_b_rating 拆分开\n",
    "            user_indexs = results[:, 0].long()   # 用户 index\n",
    "            pred_ratings = results[:, 1]         # 预测评分\n",
    "            true_ratings = results[:, 2]         # 真实评分\n",
    "\n",
    "            # 将用户 ID 转换为 numpy 数组，方便后续操作\n",
    "            user_indexs_np = user_indexs.cpu().numpy()\n",
    "            pred_ratings_np = pred_ratings.cpu().numpy()\n",
    "            true_ratings_np = true_ratings.cpu().numpy()\n",
    "\n",
    "            # 为每个用户计算 NDCG 分数\n",
    "            ndcg_scores = []\n",
    "\n",
    "            # 获取每个用户的唯一 index\n",
    "            unique_users = np.unique(user_indexs_np)\n",
    "\n",
    "            for user in unique_users:\n",
    "                # 获取当前用户的所有评分数据\n",
    "                user_true_ratings = true_ratings_np[user_indexs_np == user]\n",
    "                user_pred_ratings = pred_ratings_np[user_indexs_np == user]\n",
    "                \n",
    "                # 计算该用户的 NDCG 分数\n",
    "                if len(user_true_ratings) > 1:\n",
    "                    # ndcg@k，k取样例代码中的50\n",
    "                    ndcg = ndcg_score([user_true_ratings], [user_pred_ratings], k=50)\n",
    "                    ndcg_scores.append(torch.from_numpy(np.array(ndcg)))\n",
    "\n",
    "            # 将所有用户的 NDCG 分数存储在一个张量中\n",
    "            ndcg_scores = torch.stack(ndcg_scores)\n",
    "\n",
    "            # 计算平均 ndcg\n",
    "            ndcg_score_ = ndcg_scores.mean()\n",
    "            \n",
    "            test_loss /= i+1\n",
    "        # print protocols\n",
    "        print('At epoch [{}/{}], train_loss {:.6f}, test_loss {:.6f}, ndcg_score {:.6f}'.format(epoch+1, num_epochs, train_loss, test_loss, ndcg_score_))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda......\n",
      "At epoch [1/100], train_loss 56.504391, test_loss 51.974949, ndcg_score 0.562953\n",
      "At epoch [2/100], train_loss 47.222401, test_loss 45.304798, ndcg_score 0.562302\n",
      "At epoch [3/100], train_loss 40.493698, test_loss 40.268356, ndcg_score 0.561438\n",
      "At epoch [4/100], train_loss 35.446037, test_loss 36.343273, ndcg_score 0.560624\n",
      "At epoch [5/100], train_loss 31.528646, test_loss 33.200550, ndcg_score 0.560548\n",
      "At epoch [6/100], train_loss 28.410866, test_loss 30.621618, ndcg_score 0.560230\n",
      "At epoch [7/100], train_loss 25.867178, test_loss 28.455381, ndcg_score 0.559970\n",
      "At epoch [8/100], train_loss 23.743757, test_loss 26.592670, ndcg_score 0.559925\n",
      "At epoch [9/100], train_loss 21.922279, test_loss 24.952925, ndcg_score 0.560072\n",
      "At epoch [10/100], train_loss 20.327236, test_loss 23.473301, ndcg_score 0.560467\n",
      "At epoch [11/100], train_loss 18.894247, test_loss 22.104540, ndcg_score 0.560956\n",
      "At epoch [12/100], train_loss 17.576088, test_loss 20.808287, ndcg_score 0.561449\n",
      "At epoch [13/100], train_loss 16.339382, test_loss 19.555449, ndcg_score 0.562182\n",
      "At epoch [14/100], train_loss 15.151901, test_loss 18.326830, ndcg_score 0.563474\n",
      "At epoch [15/100], train_loss 14.000508, test_loss 17.113350, ndcg_score 0.565027\n",
      "At epoch [16/100], train_loss 12.883042, test_loss 15.915442, ndcg_score 0.566525\n",
      "At epoch [17/100], train_loss 11.801903, test_loss 14.743501, ndcg_score 0.568189\n",
      "At epoch [18/100], train_loss 10.768064, test_loss 13.614694, ndcg_score 0.570420\n",
      "At epoch [19/100], train_loss 9.797225, test_loss 12.549367, ndcg_score 0.572762\n",
      "At epoch [20/100], train_loss 8.908895, test_loss 11.565422, ndcg_score 0.575038\n",
      "At epoch [21/100], train_loss 8.110173, test_loss 10.675882, ndcg_score 0.577281\n",
      "At epoch [22/100], train_loss 7.407613, test_loss 9.886329, ndcg_score 0.579654\n",
      "At epoch [23/100], train_loss 6.801166, test_loss 9.195184, ndcg_score 0.581967\n",
      "At epoch [24/100], train_loss 6.281760, test_loss 8.595399, ndcg_score 0.583855\n",
      "At epoch [25/100], train_loss 5.839905, test_loss 8.076737, ndcg_score 0.585613\n",
      "At epoch [26/100], train_loss 5.463994, test_loss 7.628061, ndcg_score 0.587005\n",
      "At epoch [27/100], train_loss 5.142762, test_loss 7.238315, ndcg_score 0.588527\n",
      "At epoch [28/100], train_loss 4.866383, test_loss 6.897606, ndcg_score 0.589825\n",
      "At epoch [29/100], train_loss 4.626440, test_loss 6.597738, ndcg_score 0.590727\n",
      "At epoch [30/100], train_loss 4.416098, test_loss 6.331841, ndcg_score 0.591559\n",
      "At epoch [31/100], train_loss 4.230567, test_loss 6.094210, ndcg_score 0.592479\n",
      "At epoch [32/100], train_loss 4.064568, test_loss 5.880390, ndcg_score 0.593349\n",
      "At epoch [33/100], train_loss 3.915750, test_loss 5.686647, ndcg_score 0.594003\n",
      "At epoch [34/100], train_loss 3.780667, test_loss 5.510046, ndcg_score 0.594538\n",
      "At epoch [35/100], train_loss 3.658207, test_loss 5.348289, ndcg_score 0.595100\n",
      "At epoch [36/100], train_loss 3.546274, test_loss 5.199313, ndcg_score 0.595745\n",
      "At epoch [37/100], train_loss 3.442853, test_loss 5.061621, ndcg_score 0.596306\n",
      "At epoch [38/100], train_loss 3.347640, test_loss 4.933716, ndcg_score 0.596904\n",
      "At epoch [39/100], train_loss 3.259658, test_loss 4.814691, ndcg_score 0.597442\n",
      "At epoch [40/100], train_loss 3.177468, test_loss 4.703420, ndcg_score 0.598180\n",
      "At epoch [41/100], train_loss 3.100962, test_loss 4.599186, ndcg_score 0.598724\n",
      "At epoch [42/100], train_loss 3.029749, test_loss 4.501290, ndcg_score 0.599272\n",
      "At epoch [43/100], train_loss 2.962735, test_loss 4.409169, ndcg_score 0.599829\n",
      "At epoch [44/100], train_loss 2.900288, test_loss 4.322267, ndcg_score 0.600387\n",
      "At epoch [45/100], train_loss 2.841183, test_loss 4.240196, ndcg_score 0.600857\n",
      "At epoch [46/100], train_loss 2.785486, test_loss 4.162534, ndcg_score 0.601337\n",
      "At epoch [47/100], train_loss 2.732939, test_loss 4.088867, ndcg_score 0.601893\n",
      "At epoch [48/100], train_loss 2.683172, test_loss 4.018955, ndcg_score 0.602460\n",
      "At epoch [49/100], train_loss 2.636096, test_loss 3.952480, ndcg_score 0.602995\n",
      "At epoch [50/100], train_loss 2.591613, test_loss 3.889245, ndcg_score 0.603238\n",
      "At epoch [51/100], train_loss 2.549252, test_loss 3.828981, ndcg_score 0.603817\n",
      "At epoch [52/100], train_loss 2.509063, test_loss 3.771532, ndcg_score 0.604536\n",
      "At epoch [53/100], train_loss 2.471145, test_loss 3.716683, ndcg_score 0.604950\n",
      "At epoch [54/100], train_loss 2.434405, test_loss 3.664255, ndcg_score 0.605558\n",
      "At epoch [55/100], train_loss 2.399640, test_loss 3.614117, ndcg_score 0.606395\n",
      "At epoch [56/100], train_loss 2.366564, test_loss 3.566128, ndcg_score 0.606792\n",
      "At epoch [57/100], train_loss 2.334941, test_loss 3.520145, ndcg_score 0.607308\n",
      "At epoch [58/100], train_loss 2.304904, test_loss 3.476075, ndcg_score 0.607756\n",
      "At epoch [59/100], train_loss 2.276109, test_loss 3.433757, ndcg_score 0.608211\n",
      "At epoch [60/100], train_loss 2.248305, test_loss 3.393144, ndcg_score 0.608771\n",
      "At epoch [61/100], train_loss 2.221784, test_loss 3.354108, ndcg_score 0.609323\n",
      "At epoch [62/100], train_loss 2.196635, test_loss 3.316603, ndcg_score 0.609863\n",
      "At epoch [63/100], train_loss 2.172107, test_loss 3.280525, ndcg_score 0.610446\n",
      "At epoch [64/100], train_loss 2.148572, test_loss 3.245805, ndcg_score 0.611386\n",
      "At epoch [65/100], train_loss 2.126104, test_loss 3.212356, ndcg_score 0.611959\n",
      "At epoch [66/100], train_loss 2.104512, test_loss 3.180114, ndcg_score 0.612539\n",
      "At epoch [67/100], train_loss 2.083791, test_loss 3.149068, ndcg_score 0.612973\n",
      "At epoch [68/100], train_loss 2.063818, test_loss 3.119068, ndcg_score 0.613518\n",
      "At epoch [69/100], train_loss 2.044307, test_loss 3.090146, ndcg_score 0.614214\n",
      "At epoch [70/100], train_loss 2.025875, test_loss 3.062202, ndcg_score 0.614603\n",
      "At epoch [71/100], train_loss 2.007902, test_loss 3.035163, ndcg_score 0.615207\n",
      "At epoch [72/100], train_loss 1.990812, test_loss 3.009055, ndcg_score 0.615798\n",
      "At epoch [73/100], train_loss 1.974091, test_loss 2.983827, ndcg_score 0.616399\n",
      "At epoch [74/100], train_loss 1.957659, test_loss 2.959407, ndcg_score 0.616955\n",
      "At epoch [75/100], train_loss 1.942095, test_loss 2.935759, ndcg_score 0.617529\n",
      "At epoch [76/100], train_loss 1.927124, test_loss 2.912875, ndcg_score 0.618204\n",
      "At epoch [77/100], train_loss 1.912442, test_loss 2.890699, ndcg_score 0.618890\n",
      "At epoch [78/100], train_loss 1.898378, test_loss 2.869203, ndcg_score 0.619349\n",
      "At epoch [79/100], train_loss 1.884788, test_loss 2.848368, ndcg_score 0.620012\n",
      "At epoch [80/100], train_loss 1.871463, test_loss 2.828175, ndcg_score 0.620444\n",
      "At epoch [81/100], train_loss 1.858564, test_loss 2.808569, ndcg_score 0.621059\n",
      "At epoch [82/100], train_loss 1.846176, test_loss 2.789526, ndcg_score 0.621731\n",
      "At epoch [83/100], train_loss 1.834072, test_loss 2.771048, ndcg_score 0.622186\n",
      "At epoch [84/100], train_loss 1.822340, test_loss 2.753085, ndcg_score 0.622698\n",
      "At epoch [85/100], train_loss 1.811051, test_loss 2.735662, ndcg_score 0.623191\n",
      "At epoch [86/100], train_loss 1.799710, test_loss 2.718712, ndcg_score 0.623618\n",
      "At epoch [87/100], train_loss 1.789038, test_loss 2.702241, ndcg_score 0.624103\n",
      "At epoch [88/100], train_loss 1.778653, test_loss 2.686204, ndcg_score 0.624711\n",
      "At epoch [89/100], train_loss 1.768481, test_loss 2.670610, ndcg_score 0.625337\n",
      "At epoch [90/100], train_loss 1.758373, test_loss 2.655424, ndcg_score 0.626000\n",
      "At epoch [91/100], train_loss 1.748546, test_loss 2.640625, ndcg_score 0.626402\n",
      "At epoch [92/100], train_loss 1.739547, test_loss 2.626261, ndcg_score 0.626999\n",
      "At epoch [93/100], train_loss 1.730120, test_loss 2.612238, ndcg_score 0.627411\n",
      "At epoch [94/100], train_loss 1.721183, test_loss 2.598603, ndcg_score 0.628158\n",
      "At epoch [95/100], train_loss 1.712357, test_loss 2.585288, ndcg_score 0.628616\n",
      "At epoch [96/100], train_loss 1.704050, test_loss 2.572299, ndcg_score 0.629115\n",
      "At epoch [97/100], train_loss 1.695759, test_loss 2.559648, ndcg_score 0.629561\n",
      "At epoch [98/100], train_loss 1.687621, test_loss 2.547308, ndcg_score 0.629958\n",
      "At epoch [99/100], train_loss 1.679571, test_loss 2.535257, ndcg_score 0.630444\n",
      "At epoch [100/100], train_loss 1.672003, test_loss 2.523499, ndcg_score 0.631116\n"
     ]
    }
   ],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}......')\n",
    "factor_dim = 50\n",
    "net = BaseMSEMFModel(factor_dim, users_nums, movies_nums).to(device)\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "main(net, loss, train_iter, test_iter, batch_size, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
