{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是基于`Matrix Factorization`的加入正则项的普通 \\( MSE \\)的协同过滤算法来进行的电影评分的预测，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sklearn 库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_score.csv readed......\n"
     ]
    }
   ],
   "source": [
    "# 读取 movie_score_csv\n",
    "file_path = ('data/movie_score.csv')\n",
    "read_data = pd.read_csv(file_path)\n",
    "print('movie_score.csv readed......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#用户和电影的个数，构建 P Q 矩阵需要\n",
    "users_nums = len(read_data['User'].unique())\n",
    "movies_nums = len(read_data['Movie'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己的电影评分数据集类，返回用户 id的index ，电影 id的index , 以及对应的评分\n",
    "class MovieScoreDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, read_data, all_dataset = None):\n",
    "        self.read_data = read_data\n",
    "        if all_dataset == None:\n",
    "            #依据字典索引得到 user 和 movie 的 id 列表\n",
    "            self.users_unique_id_list = read_data['User'].unique()\n",
    "            self.movies_unique_id_list = read_data['Movie'].unique()\n",
    "            self.users_unique_id_list = sorted(set(self.users_unique_id_list))\n",
    "            self.movies_unique_id_list = sorted(set(self.movies_unique_id_list))\n",
    "            #依据id列表，创建 user 和 movie 的分别从 id 到 index 的转换，便于实现利用索引访问对应的矩阵 factor\n",
    "            self.user_id_to_index = {id : index for index, id in enumerate(self.users_unique_id_list)}\n",
    "            self.movie_id_to_index = {id : index for index, id in enumerate(self.movies_unique_id_list)}\n",
    "        else:\n",
    "            self.user_id_to_index = all_dataset.user_id_to_index\n",
    "            self.movie_id_to_index = all_dataset.movie_id_to_index\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 得到数据文件中index对应的一行\n",
    "        one_row = self.read_data.iloc[index]\n",
    "        user_index = self.user_id_to_index[one_row['User']]\n",
    "        movie_index = self.movie_id_to_index[one_row['Movie']]\n",
    "        u_b_rating = one_row['Rate'].astype('float32')\n",
    "        #返回 index\n",
    "        return user_index, movie_index, u_b_rating\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#划分数据集\n",
    "train_data, test_data = train_test_split(read_data, test_size=0.5, random_state=42)\n",
    "\n",
    "#创建数据集\n",
    "all_dataset = MovieScoreDataSet(read_data)\n",
    "train_dataset = MovieScoreDataSet(train_data, all_dataset)\n",
    "test_dataset = MovieScoreDataSet(test_data, all_dataset)\n",
    "\n",
    "#创建训练和测试数据迭代器，批量大小设置为256\n",
    "batch_size = 256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基础的基于MSE损失的 MF 模型\n",
    "class BaseMSEMFModel(nn.Module):\n",
    "    def __init__(self, factor_dim, users_nums, movies_nums):\n",
    "        super(BaseMSEMFModel, self).__init__()\n",
    "        self.user_matrix = nn.Embedding(users_nums, factor_dim)\n",
    "        self.movie_matrix = nn.Embedding(movies_nums, factor_dim)\n",
    "\n",
    "    # 前向传播函数，得到预测评分值   \n",
    "    def forward(self, X):\n",
    "        user_index, movie_index = X\n",
    "        user_vector = self.user_matrix(user_index)\n",
    "        movie_vector = self.movie_matrix(movie_index)\n",
    "        # 经过Embedding层出来的vector形状应该是（batch_size x factor_dim）\n",
    "        # 做点积，得到（batch_size X 1）的预测评分\n",
    "        return (user_vector * movie_vector).sum(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(net, loss, train_iter, test_iter, batch_size, device, num_epochs = 100, lr = 0.02):\n",
    "    # 定义优化器为 SGD \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    # 正则化超参数\n",
    "    lambda_1 = 0.001\n",
    "    lambda_2 = 0.001\n",
    "    # 迭代训练，每一个 epoch 打印 train loss 、test loss and test ndcg_score\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        for i, X in enumerate(train_iter):\n",
    "            user_index, movie_index, u_b_rating = X \n",
    "            # 梯度清零\n",
    "            trainer.zero_grad()\n",
    "            X = (user_index.to(device),movie_index.to(device))\n",
    "            # 模型预测\n",
    "            hat_rate = net(X)\n",
    "\n",
    "            # 计算损失，加入L2范数进行正则化\n",
    "            l = loss(hat_rate, u_b_rating.to(device)).mean() + lambda_1 * net.user_matrix.weight.norm(2) \n",
    "            + lambda_2 * net.movie_matrix.weight.norm(2)\n",
    "\n",
    "            train_loss += l\n",
    "            # 反向传播\n",
    "            l.backward()\n",
    "            # 更新参数\n",
    "            trainer.step()\n",
    "\n",
    "        train_loss /= i+1\n",
    "\n",
    "        # 测试评估\n",
    "        net.eval()\n",
    "        test_loss = 0.0\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for i, X in enumerate(test_iter):\n",
    "                user_index, movie_index, u_b_rating = X\n",
    "                X = (user_index.to(device),movie_index.to(device))\n",
    "                #得到预测得分\n",
    "                predict_rate = net(X)\n",
    "                #计算损失\n",
    "                l = loss(predict_rate, u_b_rating.to(device)).mean()\n",
    "                test_loss += l\n",
    "\n",
    "                #下面计算测试集的ndcg_socre，来评估预测的排序的效果\n",
    "                res = torch.cat([user_index.unsqueeze(1), predict_rate.cpu().unsqueeze(1), u_b_rating.unsqueeze(1)], dim = 1)\n",
    "                results.append(res)\n",
    "\n",
    "            # results变为一个(num_test_instance, 3)的张量\n",
    "            results = torch.stack(results, dim = 0)\n",
    "            results = results.flatten(start_dim=0, end_dim=1)\n",
    "            # 对不同用户分组，分别计算ndcg\n",
    "            # 将 user_index, predict_rate 和 u_b_rating 拆分开\n",
    "            user_indexs = results[:, 0].long()   # 用户 index\n",
    "            pred_ratings = results[:, 1]         # 预测评分\n",
    "            true_ratings = results[:, 2]         # 真实评分\n",
    "\n",
    "            # 将用户 ID 转换为 numpy 数组，方便后续操作\n",
    "            user_indexs_np = user_indexs.cpu().numpy()\n",
    "            pred_ratings_np = pred_ratings.cpu().numpy()\n",
    "            true_ratings_np = true_ratings.cpu().numpy()\n",
    "\n",
    "            # 为每个用户计算 NDCG 分数\n",
    "            ndcg_scores = []\n",
    "\n",
    "            # 获取每个用户的唯一 index\n",
    "            unique_users = np.unique(user_indexs_np)\n",
    "\n",
    "            for user in unique_users:\n",
    "                # 获取当前用户的所有评分数据\n",
    "                user_true_ratings = true_ratings_np[user_indexs_np == user]\n",
    "                user_pred_ratings = pred_ratings_np[user_indexs_np == user]\n",
    "                \n",
    "                # 计算该用户的 NDCG 分数\n",
    "                if len(user_true_ratings) > 1:\n",
    "                    # ndcg@k，k取样例代码中的50\n",
    "                    ndcg = ndcg_score([user_true_ratings], [user_pred_ratings], k=50)\n",
    "                    ndcg_scores.append(torch.from_numpy(np.array(ndcg)))\n",
    "\n",
    "            # 将所有用户的 NDCG 分数存储在一个张量中\n",
    "            ndcg_scores = torch.stack(ndcg_scores)\n",
    "\n",
    "            # 计算平均 ndcg\n",
    "            ndcg_score_ = ndcg_scores.mean()\n",
    "            \n",
    "            test_loss /= i+1\n",
    "        # print protocols\n",
    "        print('At epoch [{}/{}], train_loss {:.6f}, test_loss {:.6f}, ndcg_score {:.6f}'.format(epoch+1, num_epochs, train_loss, test_loss, ndcg_score_))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda......\n",
      "At epoch [1/100], train_loss 69.124916, test_loss 63.698162, ndcg_score 0.561168\n",
      "At epoch [2/100], train_loss 56.955505, test_loss 55.234718, ndcg_score 0.560146\n",
      "At epoch [3/100], train_loss 48.239502, test_loss 48.871712, ndcg_score 0.559185\n",
      "At epoch [4/100], train_loss 41.747719, test_loss 43.929375, ndcg_score 0.558560\n",
      "At epoch [5/100], train_loss 36.762157, test_loss 39.983402, ndcg_score 0.558338\n",
      "At epoch [6/100], train_loss 32.818367, test_loss 36.756794, ndcg_score 0.557809\n",
      "At epoch [7/100], train_loss 29.627974, test_loss 34.058697, ndcg_score 0.557567\n",
      "At epoch [8/100], train_loss 26.986464, test_loss 31.753389, ndcg_score 0.557069\n",
      "At epoch [9/100], train_loss 24.754396, test_loss 29.740728, ndcg_score 0.557211\n",
      "At epoch [10/100], train_loss 22.820621, test_loss 27.944860, ndcg_score 0.557274\n",
      "At epoch [11/100], train_loss 21.111460, test_loss 26.306656, ndcg_score 0.557460\n",
      "At epoch [12/100], train_loss 19.570112, test_loss 24.779947, ndcg_score 0.557744\n",
      "At epoch [13/100], train_loss 18.146332, test_loss 23.329973, ndcg_score 0.558023\n",
      "At epoch [14/100], train_loss 16.812262, test_loss 21.931765, ndcg_score 0.558955\n",
      "At epoch [15/100], train_loss 15.543498, test_loss 20.570450, ndcg_score 0.560161\n",
      "At epoch [16/100], train_loss 14.331619, test_loss 19.241259, ndcg_score 0.561266\n",
      "At epoch [17/100], train_loss 13.173671, test_loss 17.948441, ndcg_score 0.562563\n",
      "At epoch [18/100], train_loss 12.073771, test_loss 16.704332, ndcg_score 0.564516\n",
      "At epoch [19/100], train_loss 11.043669, test_loss 15.524341, ndcg_score 0.566301\n",
      "At epoch [20/100], train_loss 10.095060, test_loss 14.424621, ndcg_score 0.568263\n",
      "At epoch [21/100], train_loss 9.237142, test_loss 13.417387, ndcg_score 0.570193\n",
      "At epoch [22/100], train_loss 8.474498, test_loss 12.509451, ndcg_score 0.572390\n",
      "At epoch [23/100], train_loss 7.805930, test_loss 11.701051, ndcg_score 0.574158\n",
      "At epoch [24/100], train_loss 7.226311, test_loss 10.987342, ndcg_score 0.575600\n",
      "At epoch [25/100], train_loss 6.724880, test_loss 10.360022, ndcg_score 0.577141\n",
      "At epoch [26/100], train_loss 6.293736, test_loss 9.808963, ndcg_score 0.578457\n",
      "At epoch [27/100], train_loss 5.919841, test_loss 9.323846, ndcg_score 0.579615\n",
      "At epoch [28/100], train_loss 5.595197, test_loss 8.894771, ndcg_score 0.580711\n",
      "At epoch [29/100], train_loss 5.310816, test_loss 8.513241, ndcg_score 0.581533\n",
      "At epoch [30/100], train_loss 5.059690, test_loss 8.172011, ndcg_score 0.582566\n",
      "At epoch [31/100], train_loss 4.835737, test_loss 7.864726, ndcg_score 0.583468\n",
      "At epoch [32/100], train_loss 4.636006, test_loss 7.586558, ndcg_score 0.584009\n",
      "At epoch [33/100], train_loss 4.455471, test_loss 7.333226, ndcg_score 0.584768\n",
      "At epoch [34/100], train_loss 4.291277, test_loss 7.101201, ndcg_score 0.585246\n",
      "At epoch [35/100], train_loss 4.141435, test_loss 6.887800, ndcg_score 0.585715\n",
      "At epoch [36/100], train_loss 4.004573, test_loss 6.690555, ndcg_score 0.586194\n",
      "At epoch [37/100], train_loss 3.877837, test_loss 6.507595, ndcg_score 0.586738\n",
      "At epoch [38/100], train_loss 3.761152, test_loss 6.337379, ndcg_score 0.587229\n",
      "At epoch [39/100], train_loss 3.653004, test_loss 6.178501, ndcg_score 0.587347\n",
      "At epoch [40/100], train_loss 3.552083, test_loss 6.029715, ndcg_score 0.587635\n",
      "At epoch [41/100], train_loss 3.458621, test_loss 5.889898, ndcg_score 0.587966\n",
      "At epoch [42/100], train_loss 3.370348, test_loss 5.758399, ndcg_score 0.588483\n",
      "At epoch [43/100], train_loss 3.288618, test_loss 5.634391, ndcg_score 0.588925\n",
      "At epoch [44/100], train_loss 3.211046, test_loss 5.517241, ndcg_score 0.589346\n",
      "At epoch [45/100], train_loss 3.138487, test_loss 5.406287, ndcg_score 0.589740\n",
      "At epoch [46/100], train_loss 3.069793, test_loss 5.301237, ndcg_score 0.590136\n",
      "At epoch [47/100], train_loss 3.005291, test_loss 5.201483, ndcg_score 0.590478\n",
      "At epoch [48/100], train_loss 2.944537, test_loss 5.106642, ndcg_score 0.591110\n",
      "At epoch [49/100], train_loss 2.886430, test_loss 5.016354, ndcg_score 0.591677\n",
      "At epoch [50/100], train_loss 2.831619, test_loss 4.930331, ndcg_score 0.591986\n",
      "At epoch [51/100], train_loss 2.779866, test_loss 4.848233, ndcg_score 0.592367\n",
      "At epoch [52/100], train_loss 2.730268, test_loss 4.769891, ndcg_score 0.592830\n",
      "At epoch [53/100], train_loss 2.683312, test_loss 4.694986, ndcg_score 0.593407\n",
      "At epoch [54/100], train_loss 2.638905, test_loss 4.623312, ndcg_score 0.593952\n",
      "At epoch [55/100], train_loss 2.596111, test_loss 4.554639, ndcg_score 0.594373\n",
      "At epoch [56/100], train_loss 2.555453, test_loss 4.488872, ndcg_score 0.594830\n",
      "At epoch [57/100], train_loss 2.516678, test_loss 4.425723, ndcg_score 0.595108\n",
      "At epoch [58/100], train_loss 2.479415, test_loss 4.365124, ndcg_score 0.595463\n",
      "At epoch [59/100], train_loss 2.443977, test_loss 4.306960, ndcg_score 0.596203\n",
      "At epoch [60/100], train_loss 2.409909, test_loss 4.251027, ndcg_score 0.596465\n",
      "At epoch [61/100], train_loss 2.377527, test_loss 4.197240, ndcg_score 0.596780\n",
      "At epoch [62/100], train_loss 2.346373, test_loss 4.145487, ndcg_score 0.597241\n",
      "At epoch [63/100], train_loss 2.316690, test_loss 4.095646, ndcg_score 0.597747\n",
      "At epoch [64/100], train_loss 2.287652, test_loss 4.047639, ndcg_score 0.598205\n",
      "At epoch [65/100], train_loss 2.260014, test_loss 4.001345, ndcg_score 0.598635\n",
      "At epoch [66/100], train_loss 2.233743, test_loss 3.956658, ndcg_score 0.599077\n",
      "At epoch [67/100], train_loss 2.208412, test_loss 3.913556, ndcg_score 0.599346\n",
      "At epoch [68/100], train_loss 2.183687, test_loss 3.871944, ndcg_score 0.600010\n",
      "At epoch [69/100], train_loss 2.160196, test_loss 3.831723, ndcg_score 0.600310\n",
      "At epoch [70/100], train_loss 2.137425, test_loss 3.792890, ndcg_score 0.600762\n",
      "At epoch [71/100], train_loss 2.115445, test_loss 3.755320, ndcg_score 0.601188\n",
      "At epoch [72/100], train_loss 2.094525, test_loss 3.718989, ndcg_score 0.601483\n",
      "At epoch [73/100], train_loss 2.074010, test_loss 3.683809, ndcg_score 0.601839\n",
      "At epoch [74/100], train_loss 2.054399, test_loss 3.649765, ndcg_score 0.602327\n",
      "At epoch [75/100], train_loss 2.035222, test_loss 3.616779, ndcg_score 0.602627\n",
      "At epoch [76/100], train_loss 2.016809, test_loss 3.584836, ndcg_score 0.602964\n",
      "At epoch [77/100], train_loss 1.998992, test_loss 3.553877, ndcg_score 0.603242\n",
      "At epoch [78/100], train_loss 1.981762, test_loss 3.523825, ndcg_score 0.603434\n",
      "At epoch [79/100], train_loss 1.965167, test_loss 3.494718, ndcg_score 0.603805\n",
      "At epoch [80/100], train_loss 1.949047, test_loss 3.466420, ndcg_score 0.604273\n",
      "At epoch [81/100], train_loss 1.933429, test_loss 3.438960, ndcg_score 0.604784\n",
      "At epoch [82/100], train_loss 1.918299, test_loss 3.412308, ndcg_score 0.605174\n",
      "At epoch [83/100], train_loss 1.903528, test_loss 3.386422, ndcg_score 0.605627\n",
      "At epoch [84/100], train_loss 1.889251, test_loss 3.361283, ndcg_score 0.605932\n",
      "At epoch [85/100], train_loss 1.875454, test_loss 3.336848, ndcg_score 0.606201\n",
      "At epoch [86/100], train_loss 1.862051, test_loss 3.313073, ndcg_score 0.606549\n",
      "At epoch [87/100], train_loss 1.848985, test_loss 3.289955, ndcg_score 0.606987\n",
      "At epoch [88/100], train_loss 1.836268, test_loss 3.267452, ndcg_score 0.607508\n",
      "At epoch [89/100], train_loss 1.823830, test_loss 3.245562, ndcg_score 0.607875\n",
      "At epoch [90/100], train_loss 1.811908, test_loss 3.224267, ndcg_score 0.608097\n",
      "At epoch [91/100], train_loss 1.800081, test_loss 3.203500, ndcg_score 0.608410\n",
      "At epoch [92/100], train_loss 1.788915, test_loss 3.183295, ndcg_score 0.608794\n",
      "At epoch [93/100], train_loss 1.777756, test_loss 3.163588, ndcg_score 0.609258\n",
      "At epoch [94/100], train_loss 1.767069, test_loss 3.144405, ndcg_score 0.609658\n",
      "At epoch [95/100], train_loss 1.756436, test_loss 3.125677, ndcg_score 0.610183\n",
      "At epoch [96/100], train_loss 1.746216, test_loss 3.107451, ndcg_score 0.610704\n",
      "At epoch [97/100], train_loss 1.736194, test_loss 3.089674, ndcg_score 0.611135\n",
      "At epoch [98/100], train_loss 1.726565, test_loss 3.072314, ndcg_score 0.611553\n",
      "At epoch [99/100], train_loss 1.717054, test_loss 3.055360, ndcg_score 0.611950\n",
      "At epoch [100/100], train_loss 1.707837, test_loss 3.038842, ndcg_score 0.612383\n"
     ]
    }
   ],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}......')\n",
    "factor_dim = 64\n",
    "net = BaseMSEMFModel(factor_dim, users_nums, movies_nums).to(device)\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "main(net, loss, train_iter, test_iter, batch_size, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
