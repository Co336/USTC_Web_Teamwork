{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是基于`Matrix Factorization`的加入正则项的普通 \\( MSE \\)的协同过滤算法来进行的书籍评分的预测，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入所需库\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#sklearn库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           User     Book  Rate                       Time         Tag\n",
      "0       1398478  1467022     0  2011-03-29T12:48:35+08:00         NaN\n",
      "1       1398478  1777823     0  2011-02-02T21:58:55+08:00         NaN\n",
      "2       1398478  1902628     0  2011-01-31T15:57:58+08:00         NaN\n",
      "3       1398478  1878708     0  2011-01-26T11:27:59+08:00         NaN\n",
      "4       1398478  4238362     0  2011-01-21T13:04:15+08:00         NaN\n",
      "...         ...      ...   ...                        ...         ...\n",
      "637249  4507957  1125186     4  2009-07-04T08:02:13+08:00  张爱玲,半生缘,爱情\n",
      "637250  4507957  1002299     5  2009-07-04T08:01:28+08:00  金庸,武侠,笑傲江湖\n",
      "637251  4507957  1001136     4  2009-07-04T07:55:17+08:00     彼得・潘,童话\n",
      "637252  4507957  1021615     5  2009-07-04T07:53:54+08:00   小王子,童话,经典\n",
      "637253  4507957  1962929     5  2009-06-29T22:13:37+08:00          爱情\n",
      "\n",
      "[637254 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#读取book_score_csv\n",
    "file_path = ('data/book_score.csv')\n",
    "read_data = pd.read_csv(file_path)\n",
    "print(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#用户和书籍的个数，构建 P Q 矩阵需要\n",
    "users_nums = len(read_data['User'].unique())\n",
    "books_nums = len(read_data['Book'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己的书籍评分数据集类，返回用户 id的index ，书籍 id的index , 以及对应的评分\n",
    "class BookScoreDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, read_data, all_dataset = None):\n",
    "        self.read_data = read_data\n",
    "        if all_dataset == None:\n",
    "            #依据字典索引得到 user 和 book 的 id 列表\n",
    "            self.users_unique_id_list = read_data['User'].unique()\n",
    "            self.books_unique_id_list = read_data['Book'].unique()\n",
    "            self.users_unique_id_list = sorted(set(self.users_unique_id_list))\n",
    "            self.books_unique_id_list = sorted(set(self.books_unique_id_list))\n",
    "            #依据id列表，创建 user 和 book 的分别从 id 到 index 的转换，便于实现利用索引访问对应的矩阵 factor\n",
    "            self.user_id_to_index = {id : index for index, id in enumerate(self.users_unique_id_list)}\n",
    "            self.book_id_to_index = {id : index for index, id in enumerate(self.books_unique_id_list)}\n",
    "        else:\n",
    "            self.user_id_to_index = all_dataset.user_id_to_index\n",
    "            self.book_id_to_index = all_dataset.book_id_to_index\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 得到数据文件中index对应的一行\n",
    "        one_row = self.read_data.iloc[index]\n",
    "        user_index = self.user_id_to_index[one_row['User']]\n",
    "        book_index = self.book_id_to_index[one_row['Book']]\n",
    "        u_b_rating = one_row['Rate'].astype('float32')\n",
    "        #返回 index\n",
    "        return user_index, book_index, u_b_rating\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#划分数据集，这里我们使用助教样例代码中的train_test_split\n",
    "train_data, test_data = train_test_split(read_data, test_size=0.5, random_state=42)\n",
    "\n",
    "#创建数据集\n",
    "all_dataset = BookScoreDataSet(read_data)\n",
    "train_dataset = BookScoreDataSet(train_data, all_dataset)\n",
    "test_dataset = BookScoreDataSet(test_data, all_dataset)\n",
    "\n",
    "#创建训练和测试数据迭代器，批量大小设置为256\n",
    "batch_size = 256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基础的基于MSE损失的 MF 模型\n",
    "class BaseMSEMFModel(nn.Module):\n",
    "    def __init__(self, factor_dim, users_nums, books_nums):\n",
    "        super(BaseMSEMFModel, self).__init__()\n",
    "        self.user_matrix = nn.Embedding(users_nums, factor_dim)\n",
    "        self.book_matrix = nn.Embedding(books_nums, factor_dim)\n",
    "\n",
    "    # 前向传播函数，得到预测评分值   \n",
    "    def forward(self, X):\n",
    "        user_index, book_index = X\n",
    "        user_vector = self.user_matrix(user_index)\n",
    "        book_vector = self.book_matrix(book_index)\n",
    "        # 经过Embedding层出来的vector形状应该是（batch_size x factor_dim）\n",
    "        # 做点积，得到（batch_size X 1）的预测评分\n",
    "        return (user_vector * book_vector).sum(dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是主函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(net, loss, train_iter, test_iter, batch_size, device, num_epochs = 40, lr = 0.02):\n",
    "    # 定义优化器为 SGD \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    # 正则化超参数，这里使用样例代码中的0.001，省去调参的工作\n",
    "    lambda_1 = 0.001\n",
    "    lambda_2 = 0.001\n",
    "    # 迭代训练，每一个 epoch 打印 train loss 、test loss and test ndcg_score\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        for i, X in enumerate(train_iter):\n",
    "            user_index, book_index, u_b_rating = X \n",
    "            # 梯度清零\n",
    "            trainer.zero_grad()\n",
    "            X = (user_index.to(device),book_index.to(device))\n",
    "            # 模型预测\n",
    "            hat_rate = net(X)\n",
    "\n",
    "            # 计算损失，加入L2范数进行正则化\n",
    "            l = loss(hat_rate, u_b_rating.to(device)).mean() + lambda_1 * net.user_matrix.weight.norm(2) \n",
    "            + lambda_2 * net.book_matrix.weight.norm(2)\n",
    "\n",
    "            train_loss += l\n",
    "            # 反向传播\n",
    "            l.backward()\n",
    "            # 更新参数\n",
    "            trainer.step()\n",
    "\n",
    "        train_loss /= i+1\n",
    "\n",
    "        # 测试评估\n",
    "        net.eval()\n",
    "        test_loss = 0.0\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for i, X in enumerate(test_iter):\n",
    "                user_index, book_index, u_b_rating = X\n",
    "                X = (user_index.to(device),book_index.to(device))\n",
    "                #得到预测得分\n",
    "                predict_rate = net(X)\n",
    "                #计算损失\n",
    "                l = loss(predict_rate, u_b_rating.to(device)).mean()\n",
    "                test_loss += l\n",
    "\n",
    "                #下面计算测试集的ndcg_socre，来评估预测的排序的效果\n",
    "                res = torch.cat([user_index.unsqueeze(1), predict_rate.cpu().unsqueeze(1), u_b_rating.unsqueeze(1)], dim = 1)\n",
    "                results.append(res)\n",
    "\n",
    "            # results变为一个(num_test_instance, 3)的张量\n",
    "            results = torch.stack(results, dim = 0)\n",
    "            # 对不同用户分组，分别计算ndcg\n",
    "            # 将 user_index, predict_rate 和 u_b_rating 拆分开\n",
    "            user_indexs = results[:, 0].long()   # 用户 index\n",
    "            pred_ratings = results[:, 1]         # 预测评分\n",
    "            true_ratings = results[:, 2]         # 真实评分\n",
    "\n",
    "            # 将用户 ID 转换为 numpy 数组，方便后续操作\n",
    "            user_indexs_np = user_indexs.cpu().numpy()\n",
    "            pred_ratings_np = pred_ratings.cpu().numpy()\n",
    "            true_ratings_np = true_ratings.cpu().numpy()\n",
    "\n",
    "            # 为每个用户计算 NDCG 分数\n",
    "            ndcg_scores = []\n",
    "\n",
    "            # 获取每个用户的唯一 index\n",
    "            unique_users = np.unique(user_indexs_np)\n",
    "\n",
    "            for user in unique_users:\n",
    "                # 获取当前用户的所有评分数据\n",
    "                user_true_ratings = true_ratings_np[user_indexs_np == user]\n",
    "                user_pred_ratings = pred_ratings_np[user_indexs_np == user]\n",
    "                user_true_ratings_abs = np.abs(user_true_ratings)\n",
    "                \n",
    "                # 计算该用户的 NDCG 分数\n",
    "                if len(user_true_ratings_abs) > 1:\n",
    "                    # ndcg@k，k取样例代码中的50\n",
    "                    ndcg = ndcg_score([user_true_ratings_abs], [user_pred_ratings], k=50)\n",
    "                    ndcg_scores.append(torch.from_numpy(np.array(ndcg)))\n",
    "\n",
    "            # 将所有用户的 NDCG 分数存储在一个张量中\n",
    "            ndcg_scores = torch.stack(ndcg_scores)\n",
    "\n",
    "            # 计算平均 ndcg\n",
    "            ndcg_score_ = ndcg_scores.mean()\n",
    "            \n",
    "            test_loss /= i+1\n",
    "        # print protocols\n",
    "        print(f'At epoch {epoch}, train_loss {train_loss}, test_loss {test_loss}, ndcg_score {ndcg_score_}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0, train_loss 57.70173645019531, test_loss 54.05672073364258, ndcg_score 0.9008147040665102\n",
      "At epoch 1, train_loss 50.16964340209961, test_loss 48.829261779785156, ndcg_score 0.9018217284047589\n",
      "At epoch 2, train_loss 44.36162185668945, test_loss 44.67931365966797, ndcg_score 0.903781924963899\n",
      "At epoch 3, train_loss 39.77118682861328, test_loss 41.31207275390625, ndcg_score 0.9056073520913998\n",
      "At epoch 4, train_loss 36.056602478027344, test_loss 38.53083038330078, ndcg_score 0.9022245438047516\n",
      "At epoch 5, train_loss 33.00644302368164, test_loss 36.19734573364258, ndcg_score 0.9037756080314815\n",
      "At epoch 6, train_loss 30.457725524902344, test_loss 34.214019775390625, ndcg_score 0.9015264753276974\n",
      "At epoch 7, train_loss 28.30313491821289, test_loss 32.509212493896484, ndcg_score 0.9029600134764888\n",
      "At epoch 8, train_loss 26.458328247070312, test_loss 31.02946662902832, ndcg_score 0.9043917893602902\n",
      "At epoch 9, train_loss 24.867137908935547, test_loss 29.732990264892578, ndcg_score 0.9052625897420467\n",
      "At epoch 10, train_loss 23.482145309448242, test_loss 28.588523864746094, ndcg_score 0.9043162154820138\n",
      "At epoch 11, train_loss 22.26378059387207, test_loss 27.570642471313477, ndcg_score 0.904656841138442\n",
      "At epoch 12, train_loss 21.19001579284668, test_loss 26.659238815307617, ndcg_score 0.904387181439276\n",
      "At epoch 13, train_loss 20.235214233398438, test_loss 25.8382511138916, ndcg_score 0.9032686537337663\n",
      "At epoch 14, train_loss 19.374513626098633, test_loss 25.094482421875, ndcg_score 0.9033986397862231\n",
      "At epoch 15, train_loss 18.605649948120117, test_loss 24.416929244995117, ndcg_score 0.9016855399033705\n",
      "At epoch 16, train_loss 17.907245635986328, test_loss 23.796722412109375, ndcg_score 0.9028698420958008\n",
      "At epoch 17, train_loss 17.27143669128418, test_loss 23.22594451904297, ndcg_score 0.9046316733823393\n",
      "At epoch 18, train_loss 16.69205093383789, test_loss 22.69817352294922, ndcg_score 0.9065260671817189\n",
      "At epoch 19, train_loss 16.157358169555664, test_loss 22.207714080810547, ndcg_score 0.906052798271373\n",
      "At epoch 20, train_loss 15.66367244720459, test_loss 21.74997901916504, ndcg_score 0.9063217583362867\n",
      "At epoch 21, train_loss 15.206916809082031, test_loss 21.320741653442383, ndcg_score 0.9069879604578619\n",
      "At epoch 22, train_loss 14.782411575317383, test_loss 20.916221618652344, ndcg_score 0.9036730014048566\n",
      "At epoch 23, train_loss 14.383281707763672, test_loss 20.533355712890625, ndcg_score 0.9026202992156266\n",
      "At epoch 24, train_loss 14.008625984191895, test_loss 20.169288635253906, ndcg_score 0.9058775425694574\n",
      "At epoch 25, train_loss 13.654924392700195, test_loss 19.821514129638672, ndcg_score 0.9066901900337476\n",
      "At epoch 26, train_loss 13.318231582641602, test_loss 19.487897872924805, ndcg_score 0.9054798224351607\n",
      "At epoch 27, train_loss 12.999690055847168, test_loss 19.16632843017578, ndcg_score 0.9075032334621006\n",
      "At epoch 28, train_loss 12.692715644836426, test_loss 18.8549861907959, ndcg_score 0.9067443787247286\n",
      "At epoch 29, train_loss 12.398236274719238, test_loss 18.552282333374023, ndcg_score 0.9065787368007693\n",
      "At epoch 30, train_loss 12.116127014160156, test_loss 18.256826400756836, ndcg_score 0.9031138529930293\n",
      "At epoch 31, train_loss 11.839612007141113, test_loss 17.967273712158203, ndcg_score 0.9017508575196451\n",
      "At epoch 32, train_loss 11.572127342224121, test_loss 17.682523727416992, ndcg_score 0.9038162930894805\n",
      "At epoch 33, train_loss 11.311296463012695, test_loss 17.401538848876953, ndcg_score 0.9050694547207278\n",
      "At epoch 34, train_loss 11.054975509643555, test_loss 17.123409271240234, ndcg_score 0.9055452424656865\n",
      "At epoch 35, train_loss 10.804617881774902, test_loss 16.847299575805664, ndcg_score 0.9064634442014599\n",
      "At epoch 36, train_loss 10.557645797729492, test_loss 16.572721481323242, ndcg_score 0.9076827600069354\n",
      "At epoch 37, train_loss 10.314729690551758, test_loss 16.29897689819336, ndcg_score 0.9066751713874001\n",
      "At epoch 38, train_loss 10.074135780334473, test_loss 16.02589988708496, ndcg_score 0.9066157710007972\n",
      "At epoch 39, train_loss 9.836002349853516, test_loss 15.753053665161133, ndcg_score 0.9064257160094052\n"
     ]
    }
   ],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = BaseMSEMFModel(50, users_nums, books_nums).to(device)\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "main(net, loss, train_iter, test_iter, batch_size, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
