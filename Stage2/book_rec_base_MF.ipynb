{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是基于`Matrix Factorization`的加入正则项的普通 \\( MSE \\)的协同过滤算法来进行的书籍评分的预测，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入所需库\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#sklearn库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_score.csv readed......\n"
     ]
    }
   ],
   "source": [
    "#读取book_score_csv\n",
    "file_path = ('data/book_score.csv')\n",
    "read_data = pd.read_csv(file_path)\n",
    "print('book_score.csv readed......')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#用户和书籍的个数，构建 P Q 矩阵需要\n",
    "users_nums = len(read_data['User'].unique())\n",
    "books_nums = len(read_data['Book'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自己的书籍评分数据集类，返回用户 id的index ，书籍 id的index , 以及对应的评分\n",
    "class BookScoreDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, read_data, all_dataset = None):\n",
    "        self.read_data = read_data\n",
    "        if all_dataset == None:\n",
    "            #依据字典索引得到 user 和 book 的 id 列表\n",
    "            self.users_unique_id_list = read_data['User'].unique()\n",
    "            self.books_unique_id_list = read_data['Book'].unique()\n",
    "            self.users_unique_id_list = sorted(set(self.users_unique_id_list))\n",
    "            self.books_unique_id_list = sorted(set(self.books_unique_id_list))\n",
    "            #依据id列表，创建 user 和 book 的分别从 id 到 index 的转换，便于实现利用索引访问对应的矩阵 factor\n",
    "            self.user_id_to_index = {id : index for index, id in enumerate(self.users_unique_id_list)}\n",
    "            self.book_id_to_index = {id : index for index, id in enumerate(self.books_unique_id_list)}\n",
    "        else:\n",
    "            self.user_id_to_index = all_dataset.user_id_to_index\n",
    "            self.book_id_to_index = all_dataset.book_id_to_index\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # 得到数据文件中index对应的一行\n",
    "        one_row = self.read_data.iloc[index]\n",
    "        user_index = self.user_id_to_index[one_row['User']]\n",
    "        book_index = self.book_id_to_index[one_row['Book']]\n",
    "        u_b_rating = one_row['Rate'].astype('float32')\n",
    "        #返回 index\n",
    "        return user_index, book_index, u_b_rating\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#划分数据集\n",
    "train_data, test_data = train_test_split(read_data, test_size=0.5, random_state=42)\n",
    "\n",
    "#创建数据集\n",
    "all_dataset = BookScoreDataSet(read_data)\n",
    "train_dataset = BookScoreDataSet(train_data, all_dataset)\n",
    "test_dataset = BookScoreDataSet(test_data, all_dataset)\n",
    "\n",
    "#创建训练和测试数据迭代器，批量大小设置为256\n",
    "batch_size = 256\n",
    "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基础的基于MSE损失的 MF 模型\n",
    "class BaseMSEMFModel(nn.Module):\n",
    "    def __init__(self, factor_dim, users_nums, books_nums):\n",
    "        super(BaseMSEMFModel, self).__init__()\n",
    "        self.user_matrix = nn.Embedding(users_nums, factor_dim)\n",
    "        self.book_matrix = nn.Embedding(books_nums, factor_dim)\n",
    "\n",
    "    # 前向传播函数，得到预测评分值   \n",
    "    def forward(self, X):\n",
    "        user_index, book_index = X\n",
    "        user_vector = self.user_matrix(user_index)\n",
    "        book_vector = self.book_matrix(book_index)\n",
    "        # 经过Embedding层出来的vector形状应该是（batch_size x factor_dim）\n",
    "        # 做点积，得到（batch_size X 1）的预测评分\n",
    "        return (user_vector * book_vector).sum(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(net, loss, train_iter, test_iter, batch_size, device, num_epochs = 100, lr = 0.02):\n",
    "    # 定义优化器为 SGD \n",
    "    trainer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    # 正则化超参数\n",
    "    lambda_1 = 0.001\n",
    "    lambda_2 = 0.001\n",
    "    # 迭代训练，每一个 epoch 打印 train loss 、test loss and test ndcg_score\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        net.train()\n",
    "        train_loss = 0.0\n",
    "        for i, X in enumerate(train_iter):\n",
    "            user_index, book_index, u_b_rating = X \n",
    "            # 梯度清零\n",
    "            trainer.zero_grad()\n",
    "            X = (user_index.to(device),book_index.to(device))\n",
    "            # 模型预测\n",
    "            hat_rate = net(X)\n",
    "\n",
    "            # 计算损失，加入L2范数进行正则化\n",
    "            l = loss(hat_rate, u_b_rating.to(device)).mean() + lambda_1 * net.user_matrix.weight.norm(2) \n",
    "            + lambda_2 * net.book_matrix.weight.norm(2)\n",
    "\n",
    "            train_loss += l\n",
    "            # 反向传播\n",
    "            l.backward()\n",
    "            # 更新参数\n",
    "            trainer.step()\n",
    "\n",
    "        train_loss /= i+1\n",
    "\n",
    "        # 测试评估\n",
    "        net.eval()\n",
    "        test_loss = 0.0\n",
    "        results = []\n",
    "        with torch.no_grad():\n",
    "            for i, X in enumerate(test_iter):\n",
    "                user_index, book_index, u_b_rating = X\n",
    "                X = (user_index.to(device),book_index.to(device))\n",
    "                #得到预测得分\n",
    "                predict_rate = net(X)\n",
    "                #计算损失\n",
    "                l = loss(predict_rate, u_b_rating.to(device)).mean()\n",
    "                test_loss += l\n",
    "\n",
    "                #下面计算测试集的ndcg_socre，来评估预测的排序的效果\n",
    "                res = torch.cat([user_index.unsqueeze(1), predict_rate.cpu().unsqueeze(1), u_b_rating.unsqueeze(1)], dim = 1)\n",
    "                results.append(res)\n",
    "\n",
    "            # results变为一个(num_test_instance, 3)的张量\n",
    "            results = torch.stack(results, dim = 0)\n",
    "            results = results.flatten(start_dim=0, end_dim=1)\n",
    "            # 对不同用户分组，分别计算ndcg\n",
    "            # 将 user_index, predict_rate 和 u_b_rating 拆分开\n",
    "            user_indexs = results[:, 0].long()   # 用户 index\n",
    "            pred_ratings = results[:, 1]         # 预测评分\n",
    "            true_ratings = results[:, 2]         # 真实评分\n",
    "\n",
    "            # 将用户 ID 转换为 numpy 数组，方便后续操作\n",
    "            user_indexs_np = user_indexs.cpu().numpy()\n",
    "            pred_ratings_np = pred_ratings.cpu().numpy()\n",
    "            true_ratings_np = true_ratings.cpu().numpy()\n",
    "\n",
    "            # 为每个用户计算 NDCG 分数\n",
    "            ndcg_scores = []\n",
    "\n",
    "            # 获取每个用户的唯一 index\n",
    "            unique_users = np.unique(user_indexs_np)\n",
    "\n",
    "            for user in unique_users:\n",
    "                # 获取当前用户的所有评分数据\n",
    "                user_true_ratings = true_ratings_np[user_indexs_np == user]\n",
    "                user_pred_ratings = pred_ratings_np[user_indexs_np == user]\n",
    "                \n",
    "                # 计算该用户的 NDCG 分数\n",
    "                if len(user_true_ratings) > 1:\n",
    "                    # ndcg@k，k取样例代码中的50\n",
    "                    ndcg = ndcg_score([user_true_ratings], [user_pred_ratings], k=50)\n",
    "                    ndcg_scores.append(torch.from_numpy(np.array(ndcg)))\n",
    "\n",
    "            # 将所有用户的 NDCG 分数存储在一个张量中\n",
    "            ndcg_scores = torch.stack(ndcg_scores)\n",
    "\n",
    "            # 计算平均 ndcg\n",
    "            ndcg_score_ = ndcg_scores.mean()\n",
    "            \n",
    "            test_loss /= i+1\n",
    "        # print protocols\n",
    "        print('At epoch [{}/{}], train_loss {:.6f}, test_loss {:.6f}, ndcg_score {:.6f}'.format(epoch+1, num_epochs, train_loss, test_loss, ndcg_score_))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda......\n",
      "At epoch [1/100], train_loss 70.440384, test_loss 65.821930, ndcg_score 0.667400\n",
      "At epoch [2/100], train_loss 60.530327, test_loss 59.262489, ndcg_score 0.667761\n",
      "At epoch [3/100], train_loss 52.960773, test_loss 54.071850, ndcg_score 0.667632\n",
      "At epoch [4/100], train_loss 47.010303, test_loss 49.869713, ndcg_score 0.667926\n",
      "At epoch [5/100], train_loss 42.226208, test_loss 46.404072, ndcg_score 0.667899\n",
      "At epoch [6/100], train_loss 38.319038, test_loss 43.500454, ndcg_score 0.668056\n",
      "At epoch [7/100], train_loss 35.063854, test_loss 41.035927, ndcg_score 0.668058\n",
      "At epoch [8/100], train_loss 32.327038, test_loss 38.919479, ndcg_score 0.668276\n",
      "At epoch [9/100], train_loss 29.993603, test_loss 37.084011, ndcg_score 0.668509\n",
      "At epoch [10/100], train_loss 27.987549, test_loss 35.477509, ndcg_score 0.668477\n",
      "At epoch [11/100], train_loss 26.248569, test_loss 34.060173, ndcg_score 0.668616\n",
      "At epoch [12/100], train_loss 24.728838, test_loss 32.800732, ndcg_score 0.668787\n",
      "At epoch [13/100], train_loss 23.386208, test_loss 31.674212, ndcg_score 0.668636\n",
      "At epoch [14/100], train_loss 22.199652, test_loss 30.660212, ndcg_score 0.668870\n",
      "At epoch [15/100], train_loss 21.137636, test_loss 29.742695, ndcg_score 0.669054\n",
      "At epoch [16/100], train_loss 20.186668, test_loss 28.907782, ndcg_score 0.669109\n",
      "At epoch [17/100], train_loss 19.329653, test_loss 28.144154, ndcg_score 0.669083\n",
      "At epoch [18/100], train_loss 18.550589, test_loss 27.442457, ndcg_score 0.669049\n",
      "At epoch [19/100], train_loss 17.840496, test_loss 26.794626, ndcg_score 0.669286\n",
      "At epoch [20/100], train_loss 17.191599, test_loss 26.193779, ndcg_score 0.669435\n",
      "At epoch [21/100], train_loss 16.595472, test_loss 25.634132, ndcg_score 0.669618\n",
      "At epoch [22/100], train_loss 16.044264, test_loss 25.110569, ndcg_score 0.669658\n",
      "At epoch [23/100], train_loss 15.533111, test_loss 24.618715, ndcg_score 0.669833\n",
      "At epoch [24/100], train_loss 15.056623, test_loss 24.154753, ndcg_score 0.669962\n",
      "At epoch [25/100], train_loss 14.611212, test_loss 23.715349, ndcg_score 0.670013\n",
      "At epoch [26/100], train_loss 14.193044, test_loss 23.297409, ndcg_score 0.670080\n",
      "At epoch [27/100], train_loss 13.797995, test_loss 22.898363, ndcg_score 0.670086\n",
      "At epoch [28/100], train_loss 13.423109, test_loss 22.515787, ndcg_score 0.670307\n",
      "At epoch [29/100], train_loss 13.070539, test_loss 22.147680, ndcg_score 0.670447\n",
      "At epoch [30/100], train_loss 12.731250, test_loss 21.792198, ndcg_score 0.670424\n",
      "At epoch [31/100], train_loss 12.406180, test_loss 21.447727, ndcg_score 0.670475\n",
      "At epoch [32/100], train_loss 12.094520, test_loss 21.112707, ndcg_score 0.670574\n",
      "At epoch [33/100], train_loss 11.794951, test_loss 20.785986, ndcg_score 0.670712\n",
      "At epoch [34/100], train_loss 11.505476, test_loss 20.466230, ndcg_score 0.670809\n",
      "At epoch [35/100], train_loss 11.224097, test_loss 20.152508, ndcg_score 0.670943\n",
      "At epoch [36/100], train_loss 10.950333, test_loss 19.844048, ndcg_score 0.671065\n",
      "At epoch [37/100], train_loss 10.685040, test_loss 19.540018, ndcg_score 0.671179\n",
      "At epoch [38/100], train_loss 10.424491, test_loss 19.239857, ndcg_score 0.671290\n",
      "At epoch [39/100], train_loss 10.170412, test_loss 18.942980, ndcg_score 0.671183\n",
      "At epoch [40/100], train_loss 9.922536, test_loss 18.649019, ndcg_score 0.671261\n",
      "At epoch [41/100], train_loss 9.678706, test_loss 18.357674, ndcg_score 0.671340\n",
      "At epoch [42/100], train_loss 9.440107, test_loss 18.068684, ndcg_score 0.671537\n",
      "At epoch [43/100], train_loss 9.205507, test_loss 17.782022, ndcg_score 0.671629\n",
      "At epoch [44/100], train_loss 8.974926, test_loss 17.497534, ndcg_score 0.671782\n",
      "At epoch [45/100], train_loss 8.748793, test_loss 17.215334, ndcg_score 0.671954\n",
      "At epoch [46/100], train_loss 8.527308, test_loss 16.935492, ndcg_score 0.672143\n",
      "At epoch [47/100], train_loss 8.310828, test_loss 16.658037, ndcg_score 0.672281\n",
      "At epoch [48/100], train_loss 8.097294, test_loss 16.383234, ndcg_score 0.672436\n",
      "At epoch [49/100], train_loss 7.888662, test_loss 16.111320, ndcg_score 0.672695\n",
      "At epoch [50/100], train_loss 7.683532, test_loss 15.842538, ndcg_score 0.672863\n",
      "At epoch [51/100], train_loss 7.484644, test_loss 15.577024, ndcg_score 0.673104\n",
      "At epoch [52/100], train_loss 7.291059, test_loss 15.315153, ndcg_score 0.673211\n",
      "At epoch [53/100], train_loss 7.100318, test_loss 15.057205, ndcg_score 0.673396\n",
      "At epoch [54/100], train_loss 6.915965, test_loss 14.803306, ndcg_score 0.673537\n",
      "At epoch [55/100], train_loss 6.736660, test_loss 14.553827, ndcg_score 0.673627\n",
      "At epoch [56/100], train_loss 6.562758, test_loss 14.308895, ndcg_score 0.673791\n",
      "At epoch [57/100], train_loss 6.394347, test_loss 14.068794, ndcg_score 0.673941\n",
      "At epoch [58/100], train_loss 6.230587, test_loss 13.833715, ndcg_score 0.674048\n",
      "At epoch [59/100], train_loss 6.072371, test_loss 13.603822, ndcg_score 0.674227\n",
      "At epoch [60/100], train_loss 5.919789, test_loss 13.379251, ndcg_score 0.674358\n",
      "At epoch [61/100], train_loss 5.771742, test_loss 13.160162, ndcg_score 0.674409\n",
      "At epoch [62/100], train_loss 5.629305, test_loss 12.946638, ndcg_score 0.674514\n",
      "At epoch [63/100], train_loss 5.492451, test_loss 12.738732, ndcg_score 0.674675\n",
      "At epoch [64/100], train_loss 5.360290, test_loss 12.536490, ndcg_score 0.674766\n",
      "At epoch [65/100], train_loss 5.233810, test_loss 12.339909, ndcg_score 0.674926\n",
      "At epoch [66/100], train_loss 5.112232, test_loss 12.148956, ndcg_score 0.675000\n",
      "At epoch [67/100], train_loss 4.995303, test_loss 11.963663, ndcg_score 0.675068\n",
      "At epoch [68/100], train_loss 4.883392, test_loss 11.783928, ndcg_score 0.675176\n",
      "At epoch [69/100], train_loss 4.775446, test_loss 11.609748, ndcg_score 0.675184\n",
      "At epoch [70/100], train_loss 4.672877, test_loss 11.441007, ndcg_score 0.675377\n",
      "At epoch [71/100], train_loss 4.574266, test_loss 11.277570, ndcg_score 0.675473\n",
      "At epoch [72/100], train_loss 4.479767, test_loss 11.119396, ndcg_score 0.675564\n",
      "At epoch [73/100], train_loss 4.388845, test_loss 10.966421, ndcg_score 0.675662\n",
      "At epoch [74/100], train_loss 4.303126, test_loss 10.818405, ndcg_score 0.675804\n",
      "At epoch [75/100], train_loss 4.220575, test_loss 10.675221, ndcg_score 0.675885\n",
      "At epoch [76/100], train_loss 4.141442, test_loss 10.536829, ndcg_score 0.675963\n",
      "At epoch [77/100], train_loss 4.065706, test_loss 10.403013, ndcg_score 0.676081\n",
      "At epoch [78/100], train_loss 3.993159, test_loss 10.273683, ndcg_score 0.676241\n",
      "At epoch [79/100], train_loss 3.924233, test_loss 10.148670, ndcg_score 0.676234\n",
      "At epoch [80/100], train_loss 3.857546, test_loss 10.027884, ndcg_score 0.676424\n",
      "At epoch [81/100], train_loss 3.794099, test_loss 9.911077, ndcg_score 0.676554\n",
      "At epoch [82/100], train_loss 3.733745, test_loss 9.798199, ndcg_score 0.676662\n",
      "At epoch [83/100], train_loss 3.675768, test_loss 9.689087, ndcg_score 0.676749\n",
      "At epoch [84/100], train_loss 3.620322, test_loss 9.583591, ndcg_score 0.676839\n",
      "At epoch [85/100], train_loss 3.567024, test_loss 9.481542, ndcg_score 0.676966\n",
      "At epoch [86/100], train_loss 3.516218, test_loss 9.382885, ndcg_score 0.677020\n",
      "At epoch [87/100], train_loss 3.467428, test_loss 9.287450, ndcg_score 0.677135\n",
      "At epoch [88/100], train_loss 3.420644, test_loss 9.195194, ndcg_score 0.677216\n",
      "At epoch [89/100], train_loss 3.375815, test_loss 9.105872, ndcg_score 0.677430\n",
      "At epoch [90/100], train_loss 3.332719, test_loss 9.019464, ndcg_score 0.677529\n",
      "At epoch [91/100], train_loss 3.290985, test_loss 8.935815, ndcg_score 0.677669\n",
      "At epoch [92/100], train_loss 3.251751, test_loss 8.854814, ndcg_score 0.677798\n",
      "At epoch [93/100], train_loss 3.213759, test_loss 8.776395, ndcg_score 0.677883\n",
      "At epoch [94/100], train_loss 3.177273, test_loss 8.700371, ndcg_score 0.677845\n",
      "At epoch [95/100], train_loss 3.142132, test_loss 8.626716, ndcg_score 0.677959\n",
      "At epoch [96/100], train_loss 3.108622, test_loss 8.555282, ndcg_score 0.678061\n",
      "At epoch [97/100], train_loss 3.075834, test_loss 8.486069, ndcg_score 0.678160\n",
      "At epoch [98/100], train_loss 3.044882, test_loss 8.418920, ndcg_score 0.678234\n",
      "At epoch [99/100], train_loss 3.014521, test_loss 8.353791, ndcg_score 0.678354\n",
      "At epoch [100/100], train_loss 2.985681, test_loss 8.290593, ndcg_score 0.678439\n"
     ]
    }
   ],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}......')\n",
    "factor_dim = 64\n",
    "net = BaseMSEMFModel(factor_dim, users_nums, books_nums).to(device)\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "main(net, loss, train_iter, test_iter, batch_size, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
